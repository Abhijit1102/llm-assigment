{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "572398a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import requests\n",
    "\n",
    "# jsonschema imports\n",
    "from jsonschema import validate, ValidationError, FormatChecker\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3170c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_openai_client_for_groq():\n",
    "    \"\"\"\n",
    "    Create an OpenAI-compatible client for Groq API.\n",
    "    Requires GROQ_API_KEY in your environment.\n",
    "    \"\"\"\n",
    "    if OpenAI is None:\n",
    "        raise RuntimeError(\"OpenAI client library not installed or OpenAI class not available.\")\n",
    "    base_url = \"https://api.groq.com/openai/v1\"\n",
    "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Set GROQ_API_KEY as an environment variable before using this cell.\")\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    return client\n",
    "\n",
    "# -------------------------\n",
    "# Raw requests wrapper\n",
    "# -------------------------\n",
    "def groq_chat_request_raw(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str = \"llama-3.3-70b-versatile\",\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Send raw POST request to Groq OpenAI-compatible endpoint.\n",
    "    Returns parsed JSON on success or raises HTTPError.\n",
    "    \"\"\"\n",
    "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Set GROQ_API_KEY in env before running.\")\n",
    "    endpoint = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    # debug print (uncomment if needed):\n",
    "    # print(\"DEBUG: POST\", endpoint, \"payload model:\", model)\n",
    "    resp = requests.post(endpoint, headers=headers, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f45ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConversationHistory:\n",
    "    raw_messages: List[Dict[str, str]] = field(default_factory=list)\n",
    "    summary: Optional[str] = None\n",
    "    runs_since_last_summary: int = 0\n",
    "\n",
    "    def append(self, role: str, content: str):\n",
    "        self.raw_messages.append({\"role\": role, \"content\": content})\n",
    "        self.runs_since_last_summary += 1\n",
    "\n",
    "    def truncate_by_turns(self, n: int) -> List[Dict[str,str]]:\n",
    "        return self.raw_messages[-n:]\n",
    "\n",
    "    def truncate_by_chars(self, max_chars: int) -> List[Dict[str,str]]:\n",
    "        out = []\n",
    "        total = 0\n",
    "        for msg in reversed(self.raw_messages):\n",
    "            l = len(msg['content'])\n",
    "            if total + l > max_chars and out:\n",
    "                break\n",
    "            out.append(msg)\n",
    "            total += l\n",
    "        return list(reversed(out))\n",
    "\n",
    "    def truncate_by_words(self, max_words: int) -> List[Dict[str,str]]:\n",
    "        out = []\n",
    "        total = 0\n",
    "        for msg in reversed(self.raw_messages):\n",
    "            w = len(msg['content'].split())\n",
    "            if total + w > max_words and out:\n",
    "                break\n",
    "            out.append(msg)\n",
    "            total += w\n",
    "        return list(reversed(out))\n",
    "\n",
    "    def maybe_summarize(self, k: int, summarizer_fn):\n",
    "        \"\"\"\n",
    "        If runs_since_last_summary >= k, call summarizer_fn(conversation) -> summary string,\n",
    "        store summary and replace earlier history (we'll keep the summary as a system message).\n",
    "        \"\"\"\n",
    "        if self.runs_since_last_summary >= k:\n",
    "            combined = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.raw_messages])\n",
    "            new_summary = summarizer_fn(combined)\n",
    "            tail = self.truncate_by_turns(3)\n",
    "            self.raw_messages = [{\"role\": \"system\", \"content\": f\"SUMMARY: {new_summary}\"}] + tail\n",
    "            self.summary = new_summary\n",
    "            self.runs_since_last_summary = 0\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400f787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_summarizer(text: str, max_sentences: int = 3) -> str:\n",
    "    import re\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return \" \".join(sents[:max_sentences]) if sents else \"\"\n",
    "\n",
    "def groq_summarizer(text: str, model: str = \"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"\n",
    "    Use Groq to summarize. If no API key / client, returns a mock string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try raw request since it's generic\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise summarizer. Produce a short paragraph summary (max 60 words).\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following conversation:\\n\\n{text}\"}\n",
    "        ]\n",
    "        resp = groq_chat_request_raw(messages, model=model, max_tokens=150)\n",
    "        return resp['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[MOCK SUMMARY: {str(e)}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50edcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"email\": {\"type\": [\"string\", \"null\"], \"format\": \"email\"},\n",
    "        \"phone\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"location\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"age\": {\"type\": [\"integer\", \"null\"], \"minimum\": 0, \"maximum\": 120}\n",
    "    },\n",
    "    # we do not require every field to be present; allow nulls\n",
    "    \"required\": [\"name\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b5c93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_groq_for_extraction(user_chat: str, model: str = \"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"\n",
    "    Ask Groq to return ONLY a JSON object (name, email, phone, location, age).\n",
    "    Returns the parsed object or raises on failure.\n",
    "    If GROQ_API_KEY is not set, returns a mocked parsed dict for offline testing.\n",
    "    \"\"\"\n",
    "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        # mocked output for offline demonstration\n",
    "        # try to do a heuristic parse for better demo\n",
    "        import re\n",
    "        name = None\n",
    "        if re.search(r\"abhijit\", user_chat, re.I):\n",
    "            name = \"Abhijit Rajkumar\"\n",
    "        elif re.search(r\"aisha\", user_chat, re.I):\n",
    "            name = \"Aisha\"\n",
    "        email_m = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", user_chat)\n",
    "        phone_m = re.search(r\"(?:\\+?\\d{1,3}[-\\s]?)?\\d{6,12}\", user_chat)\n",
    "        age_m = re.search(r\"\\b(\\d{1,3})\\b\", user_chat)\n",
    "        return {\n",
    "            \"name\": name or None,\n",
    "            \"email\": email_m.group(0) if email_m else None,\n",
    "            \"phone\": phone_m.group(0) if phone_m else None,\n",
    "            \"location\": None,\n",
    "            \"age\": int(age_m.group(1)) if age_m and 0 <= int(age_m.group(1)) <= 120 else None\n",
    "        }\n",
    "\n",
    "    # If API key is present, call Groq\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You must output a JSON object and nothing else. Use null for missing fields.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Chat: ```{user_chat}```\\n\\nExtract name, email, phone, location, age. Respond only with JSON.\"}\n",
    "    ]\n",
    "    resp = groq_chat_request_raw(messages, model=model, max_tokens=200)\n",
    "    # parse\n",
    "    try:\n",
    "        content = resp['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"Unexpected response structure from Groq: \" + json.dumps(resp)[:1000])\n",
    "\n",
    "    # Some models sometimes wrap JSON in backticks or markdown; sanitize:\n",
    "    text = content.strip()\n",
    "    # find first '{' and last '}' and extract\n",
    "    first = text.find(\"{\")\n",
    "    last = text.rfind(\"}\")\n",
    "    if first != -1 and last != -1 and last > first:\n",
    "        text = text[first:last+1]\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(\"Failed to parse JSON from model output: \" + str(e) + \"\\nOutput:\\n\" + content)\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c92094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full history (raw):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Great \\u2014 what's your primary tech stack?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Python, FastAPI, HuggingFace, Qdrant, Docker.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Do you have experience with AWS and Kafka?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Yes, some with AWS and Kafka; built a small pipeline.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Truncate by last 4 turns:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Yes, some with AWS and Kafka; built a small pipeline.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Truncate by chars (max 120 chars):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Truncate by words (max 40 words):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Python, FastAPI, HuggingFace, Qdrant, Docker.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Do you have experience with AWS and Kafka?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Yes, some with AWS and Kafka; built a small pipeline.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Demonstrate periodic summarization (k=3) using naive summarizer.\n",
      "Summarization triggered: True\n",
      "History after summarization:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"SUMMARY: user: Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Extra message 1\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Extra message 2\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Extra message 3\"\n",
      "  }\n",
      "]\n",
      "Stored summary: user: Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\n",
      "\n",
      "Chat: Hi, this is Abhijit Rajkumar. My email is abhijit.raj@example.com and my phone is +91-9876543210. I'm 30 and based in Bengaluru, India.\n",
      "Extracted: {'name': 'Abhijit Rajkumar', 'email': 'abhijit.raj@example.com', 'phone': '+91-9876543210', 'location': 'Bengaluru, India', 'age': 30}\n",
      "Validation: OK ✅\n",
      "\n",
      "Chat: Hello, I'm Aisha. Reach me at aisha@example.co. I'm currently in Mumbai.\n",
      "Extracted: {'name': 'Aisha', 'email': 'aisha@example.co', 'phone': None, 'location': 'Mumbai', 'age': None}\n",
      "Validation: OK ✅\n",
      "\n",
      "Chat: This is a short message — no contact info here.\n",
      "Extracted: {'name': None, 'email': None, 'phone': None, 'location': None, 'age': None}\n",
      "Validation: OK ✅\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create history and sample messages\n",
    "    history = ConversationHistory()\n",
    "    sample_msgs = [\n",
    "        (\"user\", \"Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\"),\n",
    "        (\"assistant\", \"Great — what's your primary tech stack?\"),\n",
    "        (\"user\", \"Python, FastAPI, HuggingFace, Qdrant, Docker.\"),\n",
    "        (\"assistant\", \"Do you have experience with AWS and Kafka?\"),\n",
    "        (\"user\", \"Yes, some with AWS and Kafka; built a small pipeline.\"),\n",
    "        (\"assistant\", \"What's your preferred location?\"),\n",
    "        (\"user\", \"India or fully remote.\"),\n",
    "        (\"assistant\", \"Thanks — I'll search and prepare tailored applications.\")\n",
    "    ]\n",
    "    for role, text in sample_msgs:\n",
    "        history.append(role, text)\n",
    "\n",
    "    print(\"Full history (raw):\")\n",
    "    print(json.dumps(history.raw_messages, indent=2))\n",
    "\n",
    "    print(\"\\nTruncate by last 4 turns:\")\n",
    "    print(json.dumps(history.truncate_by_turns(4), indent=2))\n",
    "\n",
    "    print(\"\\nTruncate by chars (max 120 chars):\")\n",
    "    print(json.dumps(history.truncate_by_chars(120), indent=2))\n",
    "\n",
    "    print(\"\\nTruncate by words (max 40 words):\")\n",
    "    print(json.dumps(history.truncate_by_words(40), indent=2))\n",
    "\n",
    "    # Demonstrate k-th summarization after every 3 runs using naive summarizer\n",
    "    print(\"\\nDemonstrate periodic summarization (k=3) using naive summarizer.\")\n",
    "    for i in range(3):\n",
    "        history.append(\"user\", f\"Extra message {i+1}\")\n",
    "    triggered = history.maybe_summarize(3, lambda t: naive_summarizer(t, max_sentences=2))\n",
    "    print(\"Summarization triggered:\", triggered)\n",
    "    print(\"History after summarization:\")\n",
    "    print(json.dumps(history.raw_messages, indent=2))\n",
    "    print(\"Stored summary:\", history.summary)\n",
    "\n",
    "    # Task 2 demos: sample chats + extraction + validation\n",
    "    sample_chats = [\n",
    "        \"Hi, this is Abhijit Rajkumar. My email is abhijit.raj@example.com and my phone is +91-9876543210. I'm 30 and based in Bengaluru, India.\",\n",
    "        \"Hello, I'm Aisha. Reach me at aisha@example.co. I'm currently in Mumbai.\",\n",
    "        \"This is a short message — no contact info here.\"\n",
    "    ]\n",
    "\n",
    "    for chat in sample_chats:\n",
    "        print(\"\\nChat:\", chat)\n",
    "        try:\n",
    "            extracted = call_groq_for_extraction(chat)\n",
    "            print(\"Extracted:\", extracted)\n",
    "            # Validate\n",
    "            try:\n",
    "                validate(instance=extracted, schema=info_schema, format_checker=FormatChecker())\n",
    "                print(\"Validation: OK ✅\")\n",
    "            except ValidationError as ve:\n",
    "                print(\"Validation: FAILED ❌ -\", ve.message)\n",
    "        except Exception as e:\n",
    "            print(\"Error while extracting:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae246c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eff44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a904d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-assigment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
