{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572398a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from jsonschema import validate, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def create_openai_client_for_groq():\n",
    "    \"\"\"\n",
    "    Create an OpenAI-compatible client for Groq API.\n",
    "    Requires GROQ_API_KEY in your environment.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.groq.com/openai/v1\"\n",
    "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Set GROQ_API_KEY as an environment variable before using this cell.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    return client\n",
    "\n",
    "\n",
    "\n",
    "def groq_chat_request_raw(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str = \"llama-3.3-70b-versatile\",\n",
    "    max_tokens: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    Send raw POST request to Groq OpenAI-compatible endpoint.\n",
    "    \"\"\"\n",
    "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Set GROQ_API_KEY in env before running.\")\n",
    "\n",
    "    endpoint = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    resp = requests.post(endpoint, headers=headers, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f45ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConversationHistory:\n",
    "    raw_messages: List[Dict[str, str]] = field(default_factory=list)\n",
    "    summary: Optional[str] = None\n",
    "    runs_since_last_summary: int = 0\n",
    "\n",
    "    def append(self, role: str, content: str):\n",
    "        self.raw_messages.append({\"role\": role, \"content\": content})\n",
    "        self.runs_since_last_summary += 1\n",
    "\n",
    "    def truncate_by_turns(self, n: int) -> List[Dict[str,str]]:\n",
    "        \"\"\"Keep only last n messages (turns = messages).\"\"\"\n",
    "        return self.raw_messages[-n:]\n",
    "\n",
    "    def truncate_by_chars(self, max_chars: int) -> List[Dict[str,str]]:\n",
    "        \"\"\"Keep as many trailing messages until char limit reached.\"\"\"\n",
    "        out = []\n",
    "        total = 0\n",
    "        for msg in reversed(self.raw_messages):\n",
    "            l = len(msg['content'])\n",
    "            if total + l > max_chars and out:\n",
    "                break\n",
    "            out.append(msg)\n",
    "            total += l\n",
    "        return list(reversed(out))\n",
    "\n",
    "    def truncate_by_words(self, max_words: int) -> List[Dict[str,str]]:\n",
    "        out = []\n",
    "        total = 0\n",
    "        for msg in reversed(self.raw_messages):\n",
    "            w = len(msg['content'].split())\n",
    "            if total + w > max_words and out:\n",
    "                break\n",
    "            out.append(msg)\n",
    "            total += w\n",
    "        return list(reversed(out))\n",
    "\n",
    "    def maybe_summarize(self, k: int, summarizer_fn):\n",
    "        \"\"\"\n",
    "        If runs_since_last_summary >= k, call summarizer_fn(conversation) -> summary string,\n",
    "        store summary and replace earlier history (we'll keep the summary as a system message).\n",
    "        \"\"\"\n",
    "        if self.runs_since_last_summary >= k:\n",
    "            # create combined text to summarize\n",
    "            combined = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.raw_messages])\n",
    "            new_summary = summarizer_fn(combined)\n",
    "            # Replace the whole history with a system summary + keep last few messages (configurable).\n",
    "            # Here we keep the summary as a system message and last 3 messages to preserve context.\n",
    "            tail = self.truncate_by_turns(3)\n",
    "            self.raw_messages = [{\"role\": \"system\", \"content\": f\"SUMMARY: {new_summary}\"}] + tail\n",
    "            self.summary = new_summary\n",
    "            self.runs_since_last_summary = 0\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400f787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_summarizer(text: str, max_sentences: int = 3) -> str:\n",
    "    # Very simple: split into sentences and return first k sentences.\n",
    "    import re\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return \" \".join(sents[:max_sentences])\n",
    "\n",
    "def groq_summarizer(text: str, model: str = \"gpt-4o-mini\"):\n",
    "    # Make a conversation request to the model asking for a short summary.\n",
    "    client = None\n",
    "    try:\n",
    "        client = create_openai_client_for_groq()\n",
    "    except RuntimeError:\n",
    "        # If no GROQ_API_KEY present, return a mock summary for demonstration.\n",
    "        return \"[MOCK SUMMARY: set GROQ_API_KEY to run real summarization]\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise summarizer. Produce a short paragraph summary (max 60 words).\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize the following conversation:\\n\\n{text}\"}\n",
    "    ]\n",
    "    # Using raw requests example to ensure compatibility.\n",
    "    resp = groq_chat_request_raw(messages, model=model, max_tokens=150)\n",
    "    # Extract text depending on response format; adapt for Groq response shape.\n",
    "    # For OpenAI-style responses:\n",
    "    try:\n",
    "        return resp['choices'][0]['message']['content'].strip()\n",
    "    except Exception:\n",
    "        return \"[UNABLE TO PARSE GROQ RESPONSE]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50edcec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full history (raw):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Great \\u2014 what's your primary tech stack?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Python, FastAPI, HuggingFace, Qdrant, Docker.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Do you have experience with AWS and Kafka?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Yes, some with AWS and Kafka; built a small pipeline.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Truncate by last 4 turns:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Yes, some with AWS and Kafka; built a small pipeline.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Truncate by chars (max 120 chars):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Truncate by words (max 40 words):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Python, FastAPI, HuggingFace, Qdrant, Docker.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Do you have experience with AWS and Kafka?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Yes, some with AWS and Kafka; built a small pipeline.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"What's your preferred location?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"India or fully remote.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Thanks \\u2014 I'll search and prepare tailored applications.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Demonstrate periodic summarization (k=3) using naive summarizer.\n",
      "Summarization triggered: True\n",
      "History after summarization:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"SUMMARY: user: Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Extra message 1\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Extra message 2\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Extra message 3\"\n",
      "  }\n",
      "]\n",
      "Stored summary: user: Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\n"
     ]
    }
   ],
   "source": [
    "# Create history and feed multiple messages (simulate)\n",
    "history = ConversationHistory()\n",
    "\n",
    "sample_msgs = [\n",
    "    (\"user\", \"Hi, I'm Abhijit. I'm looking for AI/ML backend roles.\"),\n",
    "    (\"assistant\", \"Great — what's your primary tech stack?\"),\n",
    "    (\"user\", \"Python, FastAPI, HuggingFace, Qdrant, Docker.\"),\n",
    "    (\"assistant\", \"Do you have experience with AWS and Kafka?\"),\n",
    "    (\"user\", \"Yes, some with AWS and Kafka; built a small pipeline.\"),\n",
    "    (\"assistant\", \"What's your preferred location?\"),\n",
    "    (\"user\", \"India or fully remote.\"),\n",
    "    (\"assistant\", \"Thanks — I'll search and prepare tailored applications.\")\n",
    "]\n",
    "\n",
    "for role, text in sample_msgs:\n",
    "    history.append(role, text)\n",
    "\n",
    "print(\"Full history (raw):\")\n",
    "print(json.dumps(history.raw_messages, indent=2))\n",
    "\n",
    "print(\"\\nTruncate by last 4 turns:\")\n",
    "print(json.dumps(history.truncate_by_turns(4), indent=2))\n",
    "\n",
    "print(\"\\nTruncate by chars (max 120 chars):\")\n",
    "print(json.dumps(history.truncate_by_chars(120), indent=2))\n",
    "\n",
    "print(\"\\nTruncate by words (max 40 words):\")\n",
    "print(json.dumps(history.truncate_by_words(40), indent=2))\n",
    "\n",
    "# Demonstrate k-th summarization after every 3 runs:\n",
    "print(\"\\nDemonstrate periodic summarization (k=3) using naive summarizer.\")\n",
    "# Simulate additional runs to trigger summarization:\n",
    "for i in range(3):\n",
    "    history.append(\"user\", f\"Extra message {i+1}\")\n",
    "triggered = history.maybe_summarize(3, lambda t: naive_summarizer(t, max_sentences=2))\n",
    "print(\"Summarization triggered:\", triggered)\n",
    "print(\"History after summarization:\")\n",
    "print(json.dumps(history.raw_messages, indent=2))\n",
    "print(\"Stored summary:\", history.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5c93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the JSON schema to extract 5 fields: name, email, phone, location, age\n",
    "info_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
    "        \"phone\": {\"type\": \"string\"},\n",
    "        \"location\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": [\"integer\", \"null\"], \"minimum\": 0, \"maximum\": 120}\n",
    "    },\n",
    "    \"required\": [\"name\"],   # at least name required\n",
    "    \"additionalProperties\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c92094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a \"function\" description as OpenAI's function-calling style.\n",
    "# We'll ask the model to return only JSON conforming to the schema.\n",
    "\n",
    "function_description = {\n",
    "    \"name\": \"extract_user_info\",\n",
    "    \"description\": \"Extract user's personal info (name, email, phone, location, age) from a chat message. Return JSON strictly according to the provided schema.\",\n",
    "    \"parameters\": info_schema  # re-use our schema as the function parameters\n",
    "}\n",
    "\n",
    "def call_groq_for_extraction(user_chat: str, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Example request to Groq that asks the model to produce a JSON object conforming to the schema.\n",
    "    The model may support function-calling; if not, we instruct the model to output JSON only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = create_openai_client_for_groq()\n",
    "    except RuntimeError:\n",
    "        # If no API key set, return mocked responses (for testing offline)\n",
    "        return {\"name\":\"Abhijit\", \"email\":\"abhijit@example.com\", \"phone\":\"[redacted]\", \"location\":\"India\", \"age\":30}\n",
    "\n",
    "    # For maximum compatibility, instruct the model to respond ONLY with JSON following schema\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":\"You must output a JSON object and nothing else. Follow the schema precisely.\"},\n",
    "        {\"role\":\"user\", \"content\": f\"Chat: ```{user_chat}```\\n\\nExtract name, email, phone, location, age if present. If not present, use null.\"}\n",
    "    ]\n",
    "\n",
    "    # Use raw request (OpenAI-compatible endpoint)\n",
    "    resp = groq_chat_request_raw(messages, model=model, max_tokens=200)\n",
    "    # Attempt to extract the model output (varies by provider response shape)\n",
    "    try:\n",
    "        content = resp['choices'][0]['message']['content']\n",
    "        # parse JSON (model should return JSON)\n",
    "        parsed = json.loads(content)\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        # If parsing fails, return error message\n",
    "        return {\"error\": \"Unable to parse model response\", \"raw_response\": resp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae246c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat: Hi, this is Abhijit Rajkumar. My email is abhijit.raj@example.com and my phone is +91-9876543210. I'm 30 and based in Bengaluru, India.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://api.groq.com/openai/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m sample_chats:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat:\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat)\n\u001b[1;32m----> 9\u001b[0m     extracted \u001b[38;5;241m=\u001b[39m \u001b[43mcall_groq_for_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted:\u001b[39m\u001b[38;5;124m\"\u001b[39m, extracted)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Validate against schema if possible\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mcall_groq_for_extraction\u001b[1;34m(user_chat, model)\u001b[0m\n\u001b[0;32m     22\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     23\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must output a JSON object and nothing else. Follow the schema precisely.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     24\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat: ```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_chat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtract name, email, phone, location, age if present. If not present, use null.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     25\u001b[0m ]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Use raw request (OpenAI-compatible endpoint)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mgroq_chat_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Attempt to extract the model output (varies by provider response shape)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mgroq_chat_request_raw\u001b[1;34m(messages, model, max_tokens)\u001b[0m\n\u001b[0;32m     36\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     37\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(endpoint, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m---> 38\u001b[0m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mf:\\assignments_1\\llm-assigment\\.venv\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1023\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://api.groq.com/openai/v1/chat/completions"
     ]
    }
   ],
   "source": [
    "sample_chats = [\n",
    "    \"Hi, this is Abhijit Rajkumar. My email is abhijit.raj@example.com and my phone is +91-9876543210. I'm 30 and based in Bengaluru, India.\",\n",
    "    \"Hello, I'm Aisha. Reach me at aisha@example.co. I'm currently in Mumbai.\",\n",
    "    \"This is a short message — no contact info here.\"\n",
    "]\n",
    "\n",
    "for chat in sample_chats:\n",
    "    print(\"Chat:\", chat)\n",
    "    extracted = call_groq_for_extraction(chat)\n",
    "    print(\"Extracted:\", extracted)\n",
    "    # Validate against schema if possible\n",
    "    try:\n",
    "        validate(instance=extracted, schema=info_schema)\n",
    "        print(\"Validation: OK ✅\")\n",
    "    except ValidationError as ve:\n",
    "        print(\"Validation: FAILED ❌ -\", str(ve))\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0eff44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {\"id\":\"chatcmpl-bf2371de-c6be-472a-8fd8-7a65f4b69fde\",\"object\":\"chat.completion\",\"created\":1757906820,\"model\":\"llama-3.3-70b-versatile\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"It looks like you're testing our connection. Everything seems to be working fine. How can I assist you today?\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"queue_time\":0.050493941,\"prompt_tokens\":36,\"prompt_time\":0.01039972,\"completion_tokens\":24,\"completion_time\":0.048741851,\"total_tokens\":60,\"total_time\":0.059141571},\"usage_breakdown\":null,\"system_fingerprint\":\"fp_3f3b593e33\",\"x_groq\":{\"id\":\"req_01k55pjqknfnxs76tecy6xk9kn\"},\"service_tier\":\"on_demand\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "endpoint = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "data = {\n",
    "  \"model\": \"llama-3.3-70b-versatile\",   # <-- changed from llama3-8b-8192\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Test\"}\n",
    "  ],\n",
    "  \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "resp = requests.post(endpoint, headers=headers, json=data)\n",
    "print(resp.status_code, resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a904d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-assigment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
